{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89739d1b-6681-4f86-bc86-e345ff134059",
   "metadata": {},
   "source": [
    "## Reflexion\n",
    "\n",
    "The [Reflexion pattern](https://arxiv.org/pdf/2303.11366), introduced by Shinn et al., extends basic reflection by combining self-critique with external knowledge integration and structured output parsing. \n",
    "Unlike simple reflection, Reflexion allows an agent to learn from mistakes in real time while leveraging additional information.\n",
    "The workflow typically follows these steps:\n",
    "- Initial Generation: The agent produces a response along with self-critique and research queries.\n",
    "- External Research: Knowledge gaps identified during critique trigger web searches or other information retrieval.\n",
    "- Knowledge Integration: New insights are incorporated into an improved response.\n",
    "- Iterative Refinement: The agent repeats the cycle until the response meets desired quality thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5769d4-2822-41f1-8e76-28766cdeb360",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd29b6ac-62e6-491d-8d63-b6b7d8681f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import ValidationError, BaseModel, Field\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80fd3821-db95-4f84-be98-7f4f22e7839b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab71bf-894f-4f6f-909f-8c4ea68ac33a",
   "metadata": {},
   "source": [
    "### Define LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2aeadaa-6b2b-46e8-8816-b13241c9ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM - Anthropic Claude via AWS Bedrock\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    region_name=\"us-west-2\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e082cd3-a0b0-43a5-96fe-b71908274fe8",
   "metadata": {},
   "source": [
    "At its core, a Reflexion agent is built around an Actor, an agent that generates an initial response, critiques it, and then re-executes the task with improvements. Supporting this loop are a few critical sub-components:\n",
    "- Tool execution: access to external knowledge sources.\n",
    "- Initial responder: generates the first draft along with self-reflection.\n",
    "- Revisor/Revision: produces refined outputs by incorporating prior reflections.\n",
    "\n",
    "### Construct tools\n",
    "Since Reflexion requires external knowledge, we first define a tool to fetch information from the web. Here we use the TavilySearchResults tool, a wrapper around the Tavily Search API, enabling our agent to perform web searches and gather supporting evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea06c45-4574-496b-baf7-c606ae509772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "# Initialize search tool\n",
    "web_search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=web_search, max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d9ace",
   "metadata": {},
   "source": [
    "### Define the Prompt Template\n",
    "Next, let's define the prompt that will guide this actor agent's behavior. Prompts serve as the \"role description\" for an agent, specifying what it should and should not do. The agent is instructed to:\n",
    "- Provide an initial explanation.\n",
    "- Reflect and critique its own answer.\n",
    "- Generate search queries to fill knowledge gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent prompt template\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert technical educator specializing in machine learning and neural networks.\n",
    "                Current time: {time}\n",
    "                1. {primary_instruction}\n",
    "                2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "                3. Recommend search queries to research information and improve your answer.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"\\n\\n<s>Reflect on the user's original question and the\"\n",
    "            \" actions taken thus far. Respond using the {function_name} function.</reminder>\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198f2b9",
   "metadata": {},
   "source": [
    "### Enforce Structured Output\n",
    "When dealing with multi-step workflows, it's always recommended to define structured output models for each sub-agent. To ensure consistency, we define structured outputs using Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pydantic models for structured output\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    \"\"\"Generate response. Provide an answer, critique, and then follow up with search queries to improve the answer.\"\"\"\n",
    "    \n",
    "    response: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    research_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34775209",
   "metadata": {},
   "source": [
    "For this, we use Pydantic's BaseModel to define two data classes:\n",
    "- Reflection captures the self-critique, requiring the agent to highlight what information is missing and what is superfluous (unnecessary).\n",
    "- GenerateResponse structures the final output. It ensures the agent provides its main response, includes a reflection (based on the Reflection class), and supplies a list of research_queries.\n",
    "\n",
    "This structured approach guarantees that our agents produce consistent and parseable responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24906d",
   "metadata": {},
   "source": [
    "### Add Retry Logic\n",
    "Structured parsing can fail if the output doesn't match the schema. To address this, we add retry logic with schema feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b33bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent with retry logic\n",
    "class AdaptiveResponder:\n",
    "    def __init__(self, chain, output_parser):\n",
    "        self.chain = chain\n",
    "        self.output_parser = output_parser\n",
    "    \n",
    "    def generate(self, conversation_state: dict):\n",
    "        llm_response = None\n",
    "        for retry_count in range(3):\n",
    "            llm_response = self.chain.invoke(\n",
    "                {\"messages\": conversation_state[\"messages\"]}, {\"tags\": [f\"attempt:{retry_count}\"]}\n",
    "            )\n",
    "            try:\n",
    "                self.output_parser.invoke(llm_response)\n",
    "                return {\"messages\": llm_response}\n",
    "            except ValidationError as validation_error:\n",
    "                # Fix: Convert schema dict to JSON string\n",
    "                schema_json = json.dumps(self.output_parser.model_json_schema(), indent=2)\n",
    "                conversation_state = conversation_state + [\n",
    "                    llm_response,\n",
    "                    ToolMessage(\n",
    "                        content=f\"{repr(validation_error)}\\n\\nPay close attention to the function schema.\\n\\n{schema_json}\\n\\nRespond by fixing all validation errors.\",\n",
    "                        tool_call_id=llm_response.tool_calls[0][\"id\"],\n",
    "                    ),\n",
    "                ]\n",
    "        return {\"messages\": llm_response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999bbd8",
   "metadata": {},
   "source": [
    "This brings us to an important concept: retry logic with schema feedback. When structured outputs fail validation, we provide the schema and error details back to the LLM for self-correction.\n",
    "\n",
    "### Bind the Data Model\n",
    "We now bind the GenerateResponse model as a tool. This forces the LLM to output exactly in the defined structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76775a-ecc7-4c49-90f5-64be9380a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial answer chain\n",
    "initial_response_chain = actor_prompt_template.partial(\n",
    "    primary_instruction=\"Provide a detailed ~250 word explanation suitable for someone with basic programming background.\",\n",
    "    function_name=GenerateResponse.__name__,\n",
    ") | llm.bind_tools(tools=[GenerateResponse])\n",
    "\n",
    "response_parser = PydanticToolsParser(tools=[GenerateResponse])\n",
    "\n",
    "initial_responder = AdaptiveResponder(\n",
    "    chain=initial_response_chain, output_parser=response_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df2de7",
   "metadata": {},
   "source": [
    "After invoking initial_response_chain, we'll get a structured output that includes the initial answer, the self-critique, and the generated search queries. Let's try it out by testing our initial responder with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82969ab9-ed5a-44b9-a273-552dfe71c247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': AIMessage(content=\"I'll explain the key differences between supervised and unsupervised learning using the GenerateResponse function.\", additional_kwargs={'usage': {'prompt_tokens': 660, 'completion_tokens': 524, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 1184}, 'stop_reason': 'tool_use', 'thinking': {}, 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0', 'model_name': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, response_metadata={'usage': {'prompt_tokens': 660, 'completion_tokens': 524, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 1184}, 'stop_reason': 'tool_use', 'thinking': {}, 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0', 'model_name': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, id='run--316a2aa2-7cbe-4e12-8814-45121515a285-0', tool_calls=[{'name': 'GenerateResponse', 'args': {'response': \"Supervised and unsupervised learning represent two fundamental approaches in machine learning that differ primarily in how they learn from data.\\n\\nSupervised learning works with labeled data, meaning each input has a corresponding known output or target value. Think of it like learning with a teacher who provides the correct answers. For example, in a supervised learning model classifying emails as spam or not spam, each training email would be labeled with the correct classification. The algorithm learns by comparing its predictions to these known labels and adjusting its parameters to minimize errors. Common supervised learning tasks include classification (predicting categories) and regression (predicting continuous values).\\n\\nIn contrast, unsupervised learning works with unlabeled data, attempting to find hidden patterns or structures without any predefined correct answers. It's like trying to organize a pile of objects without being told how they should be grouped. For instance, clustering algorithms might group customers into distinct segments based on their purchasing behavior, without any predetermined categories. The system discovers these patterns independently by analyzing relationships and similarities in the data.\\n\\nA key practical difference is that supervised learning requires the time-consuming and often expensive process of data labeling, while unsupervised learning can work with raw, unlabeled data. However, supervised learning typically provides more precise and measurable results since there's a clear way to evaluate performance against known correct answers.\", 'reflection': {'missing': \"The explanation lacks concrete examples of popular algorithms for each type. It doesn't address semi-supervised learning as a middle ground. The explanation could benefit from discussing the specific evaluation metrics used in each approach. There's no mention of the computational complexity differences or the scale of data typically required.\", 'superfluous': 'The email spam example could be more concise. The analogy of learning with a teacher, while helpful, takes up space that could be used for more technical details.'}, 'research_queries': ['comparison of evaluation metrics in supervised vs unsupervised learning', 'popular algorithms and use cases for supervised vs unsupervised learning', 'semi-supervised learning advantages and applications']}, 'id': 'toolu_bdrk_01QbM3eR5M1wTaaRv14Kn3PG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 660, 'output_tokens': 524, 'total_tokens': 1184, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_question = \"What is the difference between supervised and unsupervised learning?\"\n",
    "initial = initial_responder.generate(\n",
    "    {\"messages\": [HumanMessage(content=example_question)]}\n",
    ")\n",
    "\n",
    "initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530faa20-2947-4973-9097-236a3052a8ed",
   "metadata": {},
   "source": [
    "### Revision\n",
    "The Revision step represents the final stage of the Reflection loop. Its purpose is to combine three critical elements: the original draft, the self-critique, and the research results,  to produce a refined, evidence-backed response.\n",
    "We start by defining a new instruction set (revise_instructions) that explicitly guides the Revisor. These instructions stress the importance of:\n",
    "- Integrating critique into the revision process\n",
    "- Adding numerical citations tied to the research evidence\n",
    "- Differentiating correlation from causation in explanations\n",
    "- Including a structured References section with clean URLs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revision instructions\n",
    "improvement_guidelines = \"\"\"Revise your previous explanation using the new information.\n",
    "    - You should use the previous critique to add important technical details to your explanation.\n",
    "    - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "    - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit).\n",
    "    - For the references field, provide a clean list of URLs only (e.g., [\"https://example.com\", \"https://example2.com\"])\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "    - Keep the explanation accessible for someone with basic programming background while being technically accurate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ddeca",
   "metadata": {},
   "source": [
    "To enforce output structure, we introduce a Pydantic schema, ImproveResponse. This class extends GenerateResponse and introduces an additional sources field, ensuring that each improved answer comes with verifiable references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43a8a5-8b84-49ed-80f3-a53d154f2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revision instructions\n",
    "class ImproveResponse(GenerateResponse):\n",
    "    \"\"\"Improve your original answer to your question. Provide an answer, reflection,\n",
    "    cite your reflection with references, and finally\n",
    "    add search queries to improve the answer.\"\"\"\n",
    "    \n",
    "    sources: list[str] = Field(\n",
    "        description=\"List of reference URLs that support your answer. Each reference should be a clean URL string.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7aa1f-f9e3-4e03-a784-2f66804c1973",
   "metadata": {},
   "source": [
    "With the schema defined, we now construct the revision chain by binding the guidelines to the LLM and parsing its outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870d7b1-c476-441b-ba87-5ec7268d80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Revision chain\n",
    "improvement_chain = actor_prompt_template.partial(\n",
    "    primary_instruction=improvement_guidelines,\n",
    "    function_name=ImproveResponse.__name__,\n",
    ") | llm.bind_tools(tools=[ImproveResponse])\n",
    "\n",
    "improvement_parser = PydanticToolsParser(tools=[ImproveResponse])\n",
    "response_improver = AdaptiveResponder(chain=improvement_chain, output_parser=improvement_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811eaec0",
   "metadata": {},
   "source": [
    "We can now test this chain by providing a full conversation history, including the initial draft, the critique, and the tool output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f18a58-a625-4b22-ab7e-fcd56a2b9d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'll use the ImproveResponse function to provide a more focused and technically precise explanation of supervised vs. unsupervised learning.\", additional_kwargs={'usage': {'prompt_tokens': 3245, 'completion_tokens': 622, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 3867}, 'stop_reason': 'tool_use', 'thinking': {}, 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0', 'model_name': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, response_metadata={'usage': {'prompt_tokens': 3245, 'completion_tokens': 622, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 3867}, 'stop_reason': 'tool_use', 'thinking': {}, 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0', 'model_name': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, id='run--3c3b46e3-9e62-4158-a3dc-0d215fa35cba-0', tool_calls=[{'name': 'ImproveResponse', 'args': {'response': 'Supervised and unsupervised learning represent distinct machine learning paradigms that differ in their learning approach and evaluation methods [1].\\n\\nSupervised learning algorithms learn from labeled training data, where each input has a corresponding target output. Common algorithms include Support Vector Machines (SVM) for classification and Linear Regression for continuous value prediction. Performance is measured through specific metrics - classification tasks use accuracy, precision, and recall, while regression tasks employ Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) [2].\\n\\nUnsupervised learning discovers hidden patterns in unlabeled data. Popular algorithms include K-means for clustering and Principal Component Analysis (PCA) for dimensionality reduction. These algorithms are evaluated using internal metrics like Silhouette Coefficient and Within-Cluster Sum Square for clustering, or Cumulative Explained Variance for dimensionality reduction [3].\\n\\nKey practical distinctions include:\\n- Data Requirements: Supervised needs labeled data; unsupervised works with raw data\\n- Evaluation: Supervised has clear performance metrics against ground truth; unsupervised uses intrinsic structure metrics\\n- Applications: Supervised excels in prediction tasks (classification/regression); unsupervised in pattern discovery (clustering/dimensionality reduction)', 'reflection': {'missing': \"The explanation could benefit from including specific real-world applications and success rates. It doesn't address the computational requirements or discuss hybrid approaches like semi-supervised learning. Could include more about the relative advantages and disadvantages of each approach.\", 'superfluous': 'The listing of evaluation metrics could be more selective - not all metrics needed to be mentioned. The explanation of algorithms could be more focused on the most commonly used ones.'}, 'research_queries': ['real-world applications and success rates of supervised vs unsupervised learning', 'computational requirements comparison supervised unsupervised learning', 'semi-supervised learning advantages over pure supervised and unsupervised approaches'], 'sources': ['https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html', 'https://medium.com/@manpreetkrbuttar/evaluation-metrics-supervised-ml-9ea9e35b2ebc', 'https://h2o.ai/blog/2022/unsupervised-learning-metrics/']}, 'id': 'toolu_bdrk_014hJaNwDhAb9Yzx7GgvCvaC', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3245, 'output_tokens': 622, 'total_tokens': 3867, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming tavily_tool is defined elsewhere\n",
    "revised = response_improver.generate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=example_question),\n",
    "            initial[\"messages\"],\n",
    "            ToolMessage(\n",
    "                tool_call_id=initial[\"messages\"].tool_calls[0][\"id\"],\n",
    "                content=json.dumps(\n",
    "                    tavily_tool.invoke(\n",
    "                        {\n",
    "                            \"query\": initial[\"messages\"].tool_calls[0][\"args\"][\n",
    "                                \"research_queries\"\n",
    "                            ][0]\n",
    "                        }\n",
    "                    )\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "revised[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d90e2-3f37-41ab-ad82-50f53ae9f6d0",
   "metadata": {},
   "source": [
    "### Create Tool Node\n",
    "The next step is to execute the tool calls inside a LangGraph workflow. While the Responder and Revisor use different schemas, they both rely on the same external tool (in this case, a search API).\n",
    "The key differentiator of Reflexion is its ability to identify knowledge gaps and actively research solutions.\n",
    "Let's implement the search integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46259dc4-bc28-4b33-9c23-19ffaa349cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tool execution function\n",
    "def execute_search_queries(research_queries: list[str], **kwargs):\n",
    "    \"\"\"Execute the generated search queries.\"\"\"\n",
    "    return tavily_tool.batch([{\"query\": search_term} for search_term in research_queries])\n",
    "\n",
    "# Tool node\n",
    "search_executor = ToolNode(\n",
    "    [\n",
    "        StructuredTool.from_function(execute_search_queries, name=GenerateResponse.__name__),\n",
    "        StructuredTool.from_function(execute_search_queries, name=ImproveResponse.__name__),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a422d-7ff3-4999-a4b5-6689f7984dc4",
   "metadata": {},
   "source": [
    "This brings us to tool integration in LangGraph workflows. The ToolNode automatically handles tool execution and result formatting, making it seamless to incorporate external knowledge sources.\n",
    "\n",
    "### Building the Graph\n",
    "\n",
    "Finally, we assemble all components: Responder, Tool Executor, and Revisor into a cyclical graph. This structure captures the iterative nature of Reflexion, where each loop strengthens the final answer.\n",
    "We first define the graph state and loop control functions:#### Construct Graph¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf703dc-94a1-4510-bf33-aed57c32b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state definition\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Helper functions for looping logic\n",
    "def get_iteration_count(message_history: list):\n",
    "    \"\"\" Counts backwards through messages until it hits a non-tool, non-AI message\n",
    "    This helps determine how many tool execution cycles have occurred recently\"\"\"\n",
    "    \n",
    "    iteration_count = 0\n",
    "    # Iterate through messages in reverse order (most recent first)\n",
    "    for message in message_history[::-1]:\n",
    "        if message.type not in {\"tool\", \"ai\"}:\n",
    "            break\n",
    "        iteration_count += 1\n",
    "    return iteration_count\n",
    "\n",
    "def determine_next_action(state: list):\n",
    "    \"\"\"\n",
    "    Conditional edge function that determines whether to continue the loop or end.\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state containing messages\n",
    "        \n",
    "    Returns:\n",
    "        str: Next node to execute (\"search_and_research\") or END to terminate\n",
    "        \n",
    "    Logic: \n",
    "    - Counts recent iterations using get_iteration_count()\n",
    "    - If we've exceeded MAXIMUM_CYCLES, stop the workflow\n",
    "    - Otherwise, continue with another tool execution cycle\n",
    "    \"\"\"\n",
    "    # in our case, we'll just stop after N plans\n",
    "    current_iterations = get_iteration_count(state[\"messages\"])\n",
    "    if current_iterations > MAXIMUM_CYCLES:\n",
    "        return END\n",
    "    return \"search_and_research\"\n",
    "\n",
    "# Graph construction\n",
    "MAXIMUM_CYCLES = 5\n",
    "workflow_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow_builder.add_node(\"create_draft\", initial_responder.generate)\n",
    "workflow_builder.add_node(\"search_and_research\", search_executor)\n",
    "workflow_builder.add_node(\"enhance_response\", response_improver.generate)\n",
    "\n",
    "# Add edges\n",
    "workflow_builder.add_edge(START, \"create_draft\")\n",
    "workflow_builder.add_edge(\"create_draft\", \"search_and_research\")\n",
    "workflow_builder.add_edge(\"search_and_research\", \"enhance_response\")\n",
    "\n",
    "# Add conditional edges for looping\n",
    "workflow_builder.add_conditional_edges(\"enhance_response\", determine_next_action, [\"search_and_research\", END])\n",
    "\n",
    "# Compile the graph\n",
    "reflexion_workflow = workflow_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1123c3da-2a8d-4e5c-9759-1605de84ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Show the agent\n",
    "display(Image(reflexion_workflow.get_graph().draw_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfbba2-bb33-48e1-9d12-296d933101db",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0ed997d-a3df-43cc-b297-41bde131868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Reflexion agent with question: How do neural networks actually learn?\n",
      "============================================================\n",
      "\n",
      "Step 0\n",
      "----------------------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How do neural networks actually learn?\n",
      "\n",
      "Step 1\n",
      "----------------------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'll provide a detailed explanation of how neural networks learn using the GenerateResponse function.\n",
      "Tool Calls:\n",
      "  GenerateResponse (toolu_bdrk_01DqGLJ3FewnonPBX697Fxu7)\n",
      " Call ID: toolu_bdrk_01DqGLJ3FewnonPBX697Fxu7\n",
      "  Args:\n",
      "    response: Neural networks learn through a process called backpropagation combined with gradient descent optimization. Here's how it works:\n",
      "\n",
      "First, a neural network starts with random weights and biases connecting its neurons. During training, it processes input data through these connections, with each neuron applying an activation function to determine its output. This forward pass generates predictions that initially will be quite wrong.\n",
      "\n",
      "The key learning happens in three steps:\n",
      "1. Error Calculation: The network compares its predictions to the actual correct answers using a loss function, which quantifies how wrong the predictions are.\n",
      "2. Backpropagation: The error is propagated backwards through the network, calculating how much each weight and bias contributed to the total error. This uses calculus (specifically the chain rule) to compute gradients that indicate how adjusting each parameter would affect the error.\n",
      "3. Parameter Updates: Using gradient descent, the network adjusts its weights and biases in small steps in the direction that reduces the error. The learning rate controls how big these steps are.\n",
      "\n",
      "This process repeats many times with different training examples. As the network processes more data, it gradually refines its parameters to minimize prediction errors. The network is essentially learning patterns in the data by adjusting its internal parameters through this systematic trial and error process, guided by mathematical optimization.\n",
      "    reflection: {'missing': \"The explanation lacks concrete examples and metaphors that would make it more accessible. It doesn't address common challenges like overfitting or vanishing gradients. The role of different activation functions and their impact on learning isn't explained. There's no mention of different optimization algorithms beyond basic gradient descent or how batch processing works.\", 'superfluous': \"The technical terms like 'backpropagation' and 'gradient descent' might be overwhelming without proper introduction. The three-step breakdown could be simplified for beginners while maintaining accuracy.\"}\n",
      "    research_queries: ['neural network learning explained with real-world analogies', 'common problems in neural network training and solutions', 'comparison of different optimization algorithms in deep learning']\n",
      "\n",
      "Step 2\n",
      "----------------------------------------\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: GenerateResponse\n",
      "\n",
      "[[{\"title\": \"Deep Learning Simplified: How NEURAL NETWORKS Work ...\", \"url\": \"https://www.youtube.com/watch?v=pLf_W4OKxEQ\", \"content\": \"- Neuron Functions: How individual neurons within the hidden layers focus on specific combinations of input parameters and how they influence the final output.\\n- Intuitive Understanding of Neural Networks: The video provides intuitive examples and analogies to help you grasp how neural networks \\\"think\\\" and make predictions.\\n- Real-World Application: Insight into how neural networks can be used in real estate for property valuation, showing the practical benefits of this technology in business. [...] Discover how neural networks work and their incredible power, in this deep dive into how they work, using a real-world example of property valuation. This tutorial walks you through the basics of neural networks, focusing on how input parameters like area, number of bedrooms, and distance to the city influence property prices. You'll learn about the crucial role of hidden layers, which add flexibility and accuracy to predictions. Perfect for beginners, this video demystifies the complex world [...] In this video, you can learn the following:\\n- Basics of Neural Networks: An introduction to how neural networks are structured and the fundamental concepts involved in their operation.\\n- Application of Neural Networks: A practical example of how a neural network can be applied to real-world scenarios, specifically in property valuation.\", \"score\": 0.72559077}, {\"title\": \"How does a neural network work? Implementation and 5 examples\", \"url\": \"https://mize.tech/blog/how-does-a-neural-network-work-implementation-and-5-examples/\", \"content\": \"For instance, a common neural network example is image recognition, where the network learns to identify objects within images based on patterns it detects.\\n\\nOn the other hand, a neural network consists of neurons interconnected like a web and these neurons are mathematical functions or models that do the computations required for classification according to a given set of rules. Through this tutorial, let’s discuss how these artificial neural networks work and their real-world usage. [...] Actually, neural networks are very beneficial in solving many of the complex real-world issues, such as:\\n\\n Image and speech recognition\\n\\nIn one of the most widespread applications of neural networks, complex pattern recognition technology relies on deep learning to analyze similarities and patterns in pixels (image recognition) and sound waves (speech recognition), allowing us to identify objects in photos and understand spoken language. [...] What is a neural network?\\n Evolution of neural networks\\n How does a neural network learn?\\n Neural network visualization\\n Neural networks vs. deep learning\\n Types of neural networks\\n Convolutional Neural Networks\\n Recurrent Neural Networks\\n Implementation of a neural network\\n When would you use a neural network?\\n Neural networks for the real world?\\n Conclusion\\n\\n17 min.\", \"score\": 0.6364215}, {\"title\": \"Machine Learning for Humans, Part 4: Neural Networks & ...\", \"url\": \"https://medium.com/machine-learning-for-humans/machine-learning-for-humans-part-4-neural-networks-deep-learning-cdad8aeae49b\", \"content\": \"## Where, why, and how deep neural networks work. Drawing inspiration from the brain. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Real-world applications.\\n\\nVishal Maini\\n\\n11 min readAug 19, 2017\\n\\n```\\nThis series is available as a full-length e-book! Download here. Free for download, contributions appreciated (paypal.me/ml4h)\\n``` [...] Pacman, to self-driving cars, to trading stocks, to (ultimately) real life and the real world. [...] Where, why, where, and how deep neural networks work. Drawing inspiration from the brain. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Real-world…\\n\\n```\\n\\nGilles Ferrand\\n\\nJan 8, 2019\\n\\nThe Mars explorer robot Curiosity is autonomously selecting inspection-worthy soil targets based on visual examination\\n\\n```\\n\\nThere is no mention of using deep learning, nor even machine learning, in either the Atlantic article or the Science paper.\\n\\n```\\n\\nTom Matte\\n\\nOct 16, 2018\", \"score\": 0.5968332}, {\"title\": \"Real-Life and Business Applications of Neural Networks\", \"url\": \"https://www.smartsheet.com/neural-network-applications?srsltid=AfmBOop402vBCYRgmzzYTSD_8dwKqScoATXP2cZyeVweifecEps6vgsA\", \"content\": \"Training: A common criticism of neural networks, particularly in robotics applications, is that excessive training for real-world operations is mandatory. One way to overcome that hurdle is by randomly shuffling training examples. Using a numerical optimization algorithm, small steps — rather than large steps — are taken to follow an example. Another way is by grouping examples in so-called mini-batches. Improving training efficiencies and convergence capabilities is an ongoing research area [...] Neural networks are fundamental to deep learning, a robust set of NN techniques that lends itself to solving abstract problems, such as bioinformatics, drug design, social network filtering, and natural language translation. Deep learning is where we will solve the most complicated issues in science and engineering, including advanced robotics. As neural networks become smarter and faster, we make advances on a daily basis.\\n\\n## Real-World and Industry Applications of Neural Networks [...] ANNs are statistical models designed to adapt and self-program by using learning algorithms in order to understand and sort out concepts, images, and photographs. For processors to do their work, developers arrange them in layers that operate in parallel. The input layer is analogous to the dendrites in the human brain’s neural network. The hidden layer is comparable to the cell body and sits between the input layer and output layer (which is akin to the synaptic outputs in the brain). The\", \"score\": 0.49919498}, {\"title\": \"How AI Works: From Neural Networks to Real-World Use\", \"url\": \"https://www.electropages.com/blog/2025/04/how-ai-works-neural-networks-real-world-use\", \"content\": \"The concept of weighted neurons and iterative training enables AI systems to learn patterns and relationships within data. By updating the weights based on errors, neural networks can be trained to recognise faces, predict outcomes, and even generate human-like responses to text-based inputs. This approach also allows models to generalise and perform well not just on training data but also on new, unseen examples. [...] ### Backpropagation: Learning from Mistakes [...] The forward pass of a neural network involves inputting data—such as an image or text—into the system. As this data flows through the network, each neuron applies a mathematical function, often referred to as an activation function, to the input signals it receives. The output of each neuron is then passed to the next layer, creating a cascading effect as the data moves through the network. The weights assigned to each connection determine the significance of the input signals, with higher\", \"score\": 0.46366343}], [{\"title\": \"What are the common challenges in training neural ...\", \"url\": \"https://milvus.io/ai-quick-reference/what-are-the-common-challenges-in-training-neural-networks\", \"content\": \"Training neural networks involves several common challenges that developers must address to achieve reliable performance. The three primary issues are data quality and quantity, optimization difficulties, and hyperparameter tuning. Each of these areas requires careful consideration to avoid wasted time and resources, as well as suboptimal model behavior. [...] issue is choosing the right loss function: a poor match between the loss and the task (e.g., using mean squared error for classification) leads to uninterpretable training signals. Debugging these problems often involves monitoring gradient magnitudes and validation metrics to identify where the training pipeline breaks down. [...] First, data-related challenges are foundational. Neural networks require large, representative datasets to generalize well, but obtaining sufficient labeled data is often difficult. For example, in medical imaging tasks, acquiring enough annotated samples can be costly or restricted by privacy concerns. Even when data is available, imbalances (e.g., 95% of samples belonging to one class) can lead models to make biased predictions. Overfitting is another risk, where a model memorizes training\", \"score\": 0.8156003}, {\"title\": \"Challenges of Deep Neural Network Training\", \"url\": \"https://www.kaggle.com/code/mbhosseini70/challenges-of-deep-neural-network-training\", \"content\": \"AbstractIntroductionChallenges in Training Deep Neural Network Models and Solutions1. Vanishing/exploding gradients1.1. Initialization1.2. Nonsaturating Activation Functions1.3. Batch Normalization1.4. Gradient Clipping2. Limited data3. Time4. OverfittingConclusionReferencesAppendixPart 1Part 2\\n\\nLicense\\n\\nThis Notebook has been released under the Apache 2.0 open source license.\\n\\nContinue exploring\", \"score\": 0.7623715}, {\"title\": \"What are some common challenges when training Neural ...\", \"url\": \"https://www.linkedin.com/pulse/what-some-common-challenges-when-training-neural-parichehr-esmailian\", \"content\": \"These are just a few of the common challenges that can arise when training neural networks. Addressing these challenges requires careful planning, experimentation, and optimization to achieve the best possible results.\\n\\nLike\\n\\nLike\\n\\nCelebrate\\n\\nSupport\\n\\nLove\\n\\nInsightful\\n\\nFunny\\n\\nComment\\n\\n4\\n\\nTo view or add a comment, sign in\\n\\n## More articles by Parichehr Esmailian\\n\\n Revolutionizing Triage Healthcare Support: How AI, E-Commerce, and NFT Technologies Are Transforming Customer Assistance [...] 7. Hyperparameter tuning: Neural networks have many hyperparameters that need to be tuned to achieve optimal performance. Finding the optimal values for these hyperparameters can be time-consuming and require extensive experimentation.\\n\\n8. Class imbalance: If the classes in the data are imbalanced, the neural network may be biased towards the majority class and perform poorly on the minority class. Techniques such as oversampling or undersampling can help to address this issue. [...] 1. Overfitting: Overfitting occurs when the neural network becomes too complex and fits the training data too closely, resulting in poor generalization to new data. This can be addressed by using techniques such as early stopping, regularization, and dropout.\", \"score\": 0.725824}, {\"title\": \"What are some well-known problems where neural networks don't ...\", \"url\": \"https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where-neural-networks-dont-do-very-well\", \"content\": \"This is more in the direction of 'what kind of problems can be solved by neural networks'. In order to train a neural network you need a large set of training data which is labelled with correct/ incorrect for the question you are interested in. So for example 'identify all pictures that have a cat on them' is very suitable for neural networks. On the other hand 'summarize the story of this toddler picture book' is very hard. Although a human can easily decide whether a given summary is any [...] However, in practice, neural networks trained with gradient descent and backpropagation face several issues and challenges, some of which are due to the training procedure and not just the architecture of the neural network or available data. [...] Furthermore, neural networks often require a lot of data in order to approximate the desired function accurately, so in cases where data is scarce neural networks may not be appropriate. Moreover, the training of neural networks (especially, deep architectures) also requires a lot of computational resources. Inference can also be sometimes problematic, when you need real-time predictions, as it can also be expensive.\", \"score\": 0.68318754}, {\"title\": \"What are the main challenges associated with training ...\", \"url\": \"https://eitca.org/artificial-intelligence/eitc-ai-arl-advanced-reinforcement-learning/deep-reinforcement-learning/deep-reinforcement-learning-agents/examination-review-deep-reinforcement-learning-agents/what-are-the-main-challenges-associated-with-training-neural-networks-using-reinforcement-learning-and-how-do-techniques-like-experience-replay-and-target-networks-address-these-challenges/\", \"content\": \"1. Instability and Divergence: One of the primary challenges in training neural networks with RL is instability and potential divergence during training. Unlike supervised learning, where the target output is fixed, in RL, the target is the future reward, which is often non-stationary and depends on the policy being learned. This can lead to oscillations or divergence in the value estimates, making it difficult for the network to converge to an optimal policy. [...] Training neural networks using reinforcement learning (RL) presents several significant challenges, primarily due to the inherent complexity and instability of the learning process. These challenges arise from the dynamic nature of the environment, the need for effective exploration, the stability of learning, and the efficiency of data usage. Techniques such as experience replay and target networks have been developed to address these issues, enhancing the performance and stability of deep [...] Training neural networks using reinforcement learning is fraught with challenges due to the dynamic and complex nature of the learning environment. Techniques such as experience replay and target networks have been developed to address key issues related to data correlation, instability, and sample efficiency. These methods, along with other advanced techniques like Double Q-learning, prioritized experience replay, actor-critic methods, entropy regularization, model-based RL, curriculum\", \"score\": 0.6811549}], [{\"title\": \"Deep Learning Optimization Algorithms - neptune.ai\", \"url\": \"https://neptune.ai/blog/deep-learning-optimization-algorithms\", \"content\": \"In this article, we’ll survey the most commonly used deep learning optimization algorithms, including Gradient Descent, Stochastic Gradient Descent, and the Adam optimizer. By the end of this article, you’ll have a clear idea of how to choose the best algorithm for training your models.\\n\\n## What is a model-optimization algorithm? [...] The optimizers used for training deep learning models are based on gradient descent, trying to shift the model’s weights towards the objective function’s minimum.\\n\\nA range of optimization algorithms is used to train deep learning models, each aiming to address a particular shortcoming of the basic gradient descent approach. [...] + Stochastic Gradient Descent (SGD) and Mini-batch Gradient Descent speed up training and are suitable for larger datasets.\\n  + AdaGrad adapts learning rates to parameters but may slow down learning over time. RMSprop and AdaDelta build on AdaGrad’s approach, addressing its diminishing learning rates, with AdaDelta removing the need for a set learning rate.\\n  + Adam combines the advantages of AdaGrad and RMSprop and is effective across a wide range of deep-learning tasks.\", \"score\": 0.8138313}, {\"title\": \"Optimization Algorithms in Deep Learning Models for Improving the ...\", \"url\": \"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4925169\", \"content\": \"In this paper we review different popular optimization algorithms for machine learning models, we then evaluate the model performance and convergence rates for each optimizer using a multilayer fully connected neural networks. Using sequential dataset of index returns (time-series data) spanning over of 20years, we demonstrate Adam and RMSprop optimizers can efficiently solve practical deep learning problems dealing with sequential datasets. We use the same parameter initialization when [...] comparing different optimization algorithms. The hyper-parameters, such as learning rate and momentum, are searched over a dense grid and the results are reported using the best hyperparameter setting.\", \"score\": 0.8003337}, {\"title\": \"Optimizers in Deep Learning: A Detailed Guide - Analytics Vidhya\", \"url\": \"https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/\", \"content\": \"Optimizers discussed include Stochastic Gradient Descent, Mini-Batch Gradient Descent, Adagrad, RMSProp, AdaDelta, and Adam.\\n By the end of the article, readers will compare different optimizers and understand their procedures [...] In deep learning, optimizers are crucial as algorithms that dynamically fine-tune a model’s parameters throughout the training process, aiming to minimize a predefined loss function. These specialized algorithms facilitate the learning process of neural networks by iteratively refining the weights and biases based on the feedback received from the data. Well-known optimizers in deep learning encompass Stochastic Gradient Descent (SGD), Adam, and RMSprop, each equipped with distinct update [...] Gradient Descent, Stochastic Gradient Descent, Mini-batch Gradient Descent, Adagrad, RMS Prop, AdaDelta, and Adam are all popular deep-learning optimizers.\\n Each optimizer has its own strengths and weaknesses, and you will choose the right one based on the specific deep-learning task and the characteristics of the data.\\n The choice of optimizer can significantly impact the speed and quality of convergence during training, as well as the final performance of the deep learning model.\", \"score\": 0.77893573}, {\"title\": \"Optimization Algorithms In Deep Learning | by Sanjithkumar - Medium\", \"url\": \"https://medium.com/@sanjithkumar986/optimization-algorithms-in-deep-learning-0b822c0d18c6\", \"content\": \"The specific deep learning task: Different tasks might have different optimization landscapes, and some algorithms might be better suited for specific problems.\\n Dataset size and characteristics: The size and characteristics of the dataset can influence the choice of optimizer. For example, Adam might be more efficient for large datasets compared to AdaGrad. [...] To address the limitations of gradient descent, several advanced optimization algorithms have been developed specifically for deep learning. These algorithms aim to accelerate the training process and improve the model’s performance. Three such algorithms are AdaGrad, RMSprop, and Adam, which we will discuss in this blog post.\\n\\n# Stochastic Gradient Descent (SGD) [...] Efficient learning rate adaptation: Adam combines the adaptive learning rate capabilities of AdaGrad and RMSprop with momentum to efficiently adjust learning rates for each parameter, leading to faster convergence and potentially better performance compared to its predecessors.\\n Less sensitive to learning rate: Similar to SGD with momentum, Adam is less sensitive to the specific learning rate chosen, making it easier to tune hyperparameters.\", \"score\": 0.7730283}, {\"title\": \"Toward Understanding Why Adam Converges Faster Than SGD for ...\", \"url\": \"https://arxiv.org/abs/2306.00204\", \"content\": \"> Abstract:While stochastic gradient descent (SGD) is still the most popular optimization algorithm in deep learning, adaptive algorithms such as Adam have established empirical advantages over SGD in some deep learning applications such as training transformers. However, it remains a question that why Adam converges significantly faster than SGD in these scenarios. In this paper, we propose one explanation of why Adam converges faster than SGD using a new concept directional sharpness. We\", \"score\": 0.7440808}]]\n",
      "\n",
      "Step 3\n",
      "----------------------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'll provide an improved explanation of how neural networks learn using the ImproveResponse function.\n",
      "Tool Calls:\n",
      "  ImproveResponse (toolu_bdrk_0169RN8TSNRTCE2qrMEi9Qkx)\n",
      " Call ID: toolu_bdrk_0169RN8TSNRTCE2qrMEi9Qkx\n",
      "  Args:\n",
      "    response: Neural networks learn through a systematic process that mimics how our brains adapt to new information. The learning process involves three key mechanisms:\n",
      "\n",
      "1. Forward Pass: The network processes input data through layers of interconnected neurons, each applying mathematical transformations. Initially, these connections have random weights, similar to an untrained brain [1].\n",
      "\n",
      "2. Error Assessment: The network compares its predictions against correct answers using a loss function. For example, in image recognition, it might check how far its guess of \"cat\" was from the actual label [2].\n",
      "\n",
      "3. Learning Update: Through backpropagation, the network calculates how each connection contributed to errors. Using optimization algorithms like Adam or SGD, it adjusts these connections to reduce future errors [3]. Think of this like strengthening neural pathways that lead to correct answers.\n",
      "\n",
      "Modern networks use sophisticated techniques to overcome common challenges. Batch normalization prevents internal signals from becoming too large or small, while dropout randomly deactivates neurons during training to prevent over-reliance on any single connection [4]. Learning rates control how quickly the network updates its knowledge, with adaptive methods like Adam automatically adjusting these rates for optimal learning [5].\n",
      "    reflection: {'missing': \"The explanation could benefit from more specific examples of activation functions and their purposes. It doesn't address how different types of neural networks (CNNs, RNNs) learn differently. The mathematical foundation of gradient descent could be explained more clearly.\", 'superfluous': 'The brain analogy, while helpful for beginners, takes up space that could be used for more technical details. The explanation of batch normalization and dropout could be more concise.'}\n",
      "    research_queries: ['comparison of activation functions in neural networks', 'mathematical principles of gradient descent optimization', 'differences between CNN and RNN learning processes']\n",
      "    sources: ['https://www.youtube.com/watch?v=pLf_W4OKxEQ', 'https://mize.tech/blog/how-does-a-neural-network-work-implementation-and-5-examples/', 'https://neptune.ai/blog/deep-learning-optimization-algorithms', 'https://www.smartsheet.com/neural-network-applications', 'https://milvus.io/ai-quick-reference/what-are-the-common-challenges-in-training-neural-networks']\n",
      "\n",
      "Step 4\n",
      "----------------------------------------\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: ImproveResponse\n",
      "\n",
      "[[{\"title\": \"Review and Comparison of Commonly Used Activation ...\", \"url\": \"https://arxiv.org/pdf/2010.09458\", \"content\": \"by T Szandała · 2020 · Cited by 592 — These types of activation functions used to be very common among artificial neural networks. The output of these activation functions is not linear. Thus, the\", \"score\": 0.98519}, {\"title\": \"Understanding Different Activation Functions\", \"url\": \"https://medium.com/@deepyachowdary/understanding-different-activation-functions-6aed5ed1c785\", \"content\": \"Nonlinear activation functions enable neural networks to capture complex structures and solve complex problems. Range: Activation functions\", \"score\": 0.98399}, {\"title\": \"Activation Functions in Neural Networks [12 Types & Use ...\", \"url\": \"https://www.v7labs.com/blog/neural-networks-activation-functions\", \"content\": \"What is a neural network activation function and how does it work? What is a Neural Network Activation Function? Why do Neural Networks Need an Activation Function? * What is a Neural Networks Activation Function? * Types of Neural Networks Activation Functions * 10 Non-Linear Neural Networks Activation Functions ## What is a Neural Network Activation Function? ## 3 Types of Neural Networks Activation Functions * All layers of the neural network will collapse into one if a linear activation function is used. So, essentially, a linear activation function turns the neural network into just one layer. Now, let’s have a look at ten different non-linear neural networks activation functions and their characteristics. ### 10 Non-Linear Neural Networks Activation Functions\", \"score\": 0.98238}, {\"title\": \"Activation functions in Neural Networks\", \"url\": \"https://www.geeksforgeeks.org/machine-learning/activation-functions-neural-networks/\", \"content\": \"Activation functions help neural networks to model highly complex data distributions and solve advanced deep learning tasks. Adding non-linear activation functions introduce flexibility and enable the network to learn more complex and abstract patterns from data. This gives the final output of the network after applying the sigmoid activation function in output layers, introducing the desired non-linearity. No matter how many layers the neural network contains if they all use linear activation functions the output is a linear combination of the input. Linear activation functions are useful for specific tasks but must be combined with non-linear functions to enhance the neural network’s learning and predictive capabilities. * ****Nature****: It is a non-linear activation function, allowing neural networks to learn complex patterns and making backpropagation more efficient.\", \"score\": 0.97912}, {\"title\": \"Comparison of Sigmoid, Tanh and ReLU Activation Functions\", \"url\": \"https://www.aitude.com/comparison-of-sigmoid-tanh-and-relu-activation-functions/\", \"content\": \"In Artificial Neural network(ANN), activation functions are the most informative ingredient of Deep Learning which is fundamentally used for to determine the output of the deep learning models. But in this blog we will put more concentrate on the Sigmoid Activation Function, Tanh Activation Function and Relu (Rectified Linear Unit) Activation Function because these Activation Functions are mostly used in ANN (Artificial Neural Network) and deep learning. Here we have discussed the working of the Artificial Neural Network and understand the functionality of sigmoid, hyperbolic Tangent (TanH), and ReLu (Rectified Linear Unit) Activation function and we also compared in a very easy manner.\", \"score\": 0.9776}], [{\"title\": \"[PDF] Optimization of Mathematical Functions Using Gradient Descent ...\", \"url\": \"https://opus.govst.edu/cgi/viewcontent.cgi?article=1001&context=theses_math\", \"content\": \"which are discuss ed in later sections of this paper. OPTIMIZATION OF MATHEMATICAL FUNCTIONS 7\\n\\n2. Gradient Descent \\n\\nThe Gradient Descent algorithm is an optimization algorithm for minimizing convex \\n\\nfunctions. It works on the principle that if we continue to move in the direction of the steepest \\n\\ndescent from a pa rticular starting point which may be randomly initialized, then we reach the \\n\\nminimum of the function, provided that the function to be minimized is differentiable at [...] the optimization algorithms based on gradient descent. The basic idea behind the Gradient OPTIMIZATION OF MATHEMATICAL FUNCTIONS 6\\n\\nDescent algorithm is to minimize the value of a convex function by moving along the \\n\\ngradients. This involves calculating the direction of the gradients from the given point and \\n\\nthen taking a step along the direction of steepest descent. As a simple explanation, Gradient \\n\\nDescent can be considered as moving down the hill starting from any point. If we keep [...] can be used for minimizing as well as maximizing a function. When it comes to constrained \\n\\noptimization, not all of these algorithms are applicable. The goal of discussion here is \\n\\noptimizing a function to find its minimum, when we know that the function is convex in \\n\\nnature. \\n\\nGradient Descent algorithm as the name suggests, works on the principle of descending along \\n\\nthe direction given by gradient to attain the minimum value. However, sometimes, this may\", \"score\": 0.8755427}, {\"title\": \"Gradient descent - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Gradient_descent\", \"content\": \"Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function. [...] Gradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades. [...] The gradient descent can be modified via momentums (Nesterov, Polyak, and Frank–Wolfe) and heavy-ball parameters (exponential moving averages and positive-negative momentum). The main examples of such optimizers are Adam, DiffGrad, Yogi, AdaBelief, etc.\", \"score\": 0.76992935}, {\"title\": \"Understanding the Mathematics behind Gradient Descent.\", \"url\": \"https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e/\", \"content\": \"Skip to content\\n\\nPublish AI, ML & data-science insights to a global community of data professionals.\\n\\nSubmit an Article\\n\\n LinkedIn\\n X\\n\\nData Science\\n\\n# Understanding the Mathematics behind Gradient Descent.\\n\\nA simple mathematical intuition behind one of the commonly used optimisation algorithms in Machine Learning.\\n\\nParul Pandey\\n\\n10 min read\\n\\n> \\\"Premature optimization is the root of all evil.\\\"\\n> ― Donald Ervin Knuth [...] The point of this article was to demonstrate the concept of gradient descent. We used gradient descent as our optimization strategy for linear regression. by drawing the line of best fit to measure the relationship between student heights and weights. However, it is important to note here that the linear regression example has been chosen for simplicity but can be used with other Machine Learning techniques.\\n\\nWritten By\\n\\nParul Pandey\\n\\nSee all from Parul Pandey [...] > Hence,to solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.\\n\\n## Conclusion\", \"score\": 0.7099254}, {\"title\": \"Understanding Gradient Descent Algorithm and the Maths Behind It\", \"url\": \"https://www.analyticsvidhya.com/blog/2021/08/understanding-gradient-descent-algorithm-and-the-maths-behind-it/\", \"content\": \"The basic principle of gradient descent is to choose the step size (also called as learning rate) appropriately so that we can get close to the exact solution. So learning rate basically controls how big a step we take downhill.\\n\\nThe gradient descent update rule is given as follows :\\n\\nGradient Descent can be summarized using the formula,\\n\\nImage Source: Created by Author\\n\\nWe repeatedly calculate this until convergence.\\n\\nLet’s see how we got this formula. [...] Again you will look around to find the direction to take the next baby step in order to go downhill. If you take that step, you will land upon another point on the hill.\\n\\nThis way thinking about what direction to go and take the step will be continued until you reach downhill which is the global minimum.\\n\\nThe Core of Gradient Descent Optimisation Algorithm [...] The Gradient Descent Algorithm helps improve machine learning models by reducing errors. It works by adjusting the model’s parameters step by step, following the slope of a cost or loss function, which measures how far the model’s predictions are from the actual values. Starting with a basic model, Gradient Descent refines it by continuously lowering this error. The key idea is to minimize the cost function, making the model’s predictions more accurate over time. This method is essential for\", \"score\": 0.6762998}, {\"title\": \"Mathematics Behind Gradient Descent | by iManassa | Geek Culture\", \"url\": \"https://medium.com/geekculture/mathematics-behind-gradient-descent-f2a49a0b714f\", \"content\": \"Since now we know what are derivatives, partial derivatives and cost functions, we can now learn about the concept of gradient descent.\\n\\n## Get iManassa’s stories in your inbox\\n\\nJoin Medium for free to get updates from this writer.\\n\\nGradient descent is a first-order, iterative optimization algorithm to find a local/global minima of a differentiable function. We do know that we have to find the line that gives us the least cost and this is done using gradient descent. But how does this happen? [...] At the end of this article, you would have learnt about the mathematical intuition behind the gradient descent algorithm and how it works in machine learning. You would have also learnt about the three variants of the algorithm.\\n\\nI hope it has increased your understanding of gradient descent and given you a clear picture of what happens behind the hood.\\n\\nThank you for reading and if you found the article useful, please do share it!\\n\\nGradient Descent\\n\\nOptimization\\n\\nData Science [...] Gradient Descent is an optimization algorithm that is used to find the values of the parameters of a function (linear regression, logistic regression etc.) that is used to reduce a cost function. In very simple terms, it helps us to find the best fit line. How does it do this? Before going into this, we have to know what some terms mean.\\n\\n Derivatives\", \"score\": 0.63316137}], [{\"title\": \"CNN vs. RNN: How are they different? - TechTarget\", \"url\": \"https://www.techtarget.com/searchenterpriseai/feature/CNN-vs-RNN-How-they-differ-and-where-they-overlap\", \"content\": \"CNNs handle spatial data like images, while RNNs handle temporal data like text. CNNs have fixed input/output; RNNs have variable input/output.\", \"score\": 0.98536}, {\"title\": \"Difference between ANN, CNN and RNN - GeeksforGeeks\", \"url\": \"https://www.geeksforgeeks.org/deep-learning/difference-between-ann-cnn-and-rnn/\", \"content\": \"Deep learning mimics neural networks of the human brain, it enables computers to autonomously uncover patterns and make informed decisions from vast amounts of unstructured data. Convolutional Neural Networks (CNNs) are deep learning models designed to process data with a grid-like topology such as images. In machine learning Recurrent Neural Networks (RNNs) are essential for tasks involving sequential data such as text, speech and time-series analysis. PyTorch is an open-source deep learning framework designed to simplify the process of building neural networks and machine learning models. Convolutional Neural Networks (CNNs) are a type of deep learning model specifically designed for processing images. Deep learning is a part of machine learning that is based on the artificial neural network with multiple layers to learn from and make predictions on data.\", \"score\": 0.97778}, {\"title\": \"RNN vs CNN at a high level\", \"url\": \"https://datascience.stackexchange.com/questions/11619/rnn-vs-cnn-at-a-high-level\", \"content\": \"The reason the image appears in the article is that it demonstrates the relative flexibility of RNNs: An RNN can be applied to all the different types of problems shown (e.g. it can be used in language translation problems which match the 4th item), whilst a feed-forward network only applies to problems matching the first image. 2. RNN, unlike feedforward neural networks, can use their internal memory to process arbitrary sequences of inputs. Based on situations sometimes we can leverage both (CNN and RNN) of them in one model (say describing an image in a sentence) or even surprisingly we can interchange them - for e.g. use CNNs where RNN's should have been used..\", \"score\": 0.97621}, {\"title\": \"12 Types of Neural Networks in Deep Learning - Analytics Vidhya\", \"url\": \"https://www.analyticsvidhya.com/blog/2020/02/cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning/\", \"content\": \"Explore the architecture, training, and prediction processes of 12 types of neural networks in deep learning, including CNNs, LSTMs,\", \"score\": 0.97518}, {\"title\": \"RNN vs CNN for Deep Learning: Let's Learn the Difference\", \"url\": \"https://www.exxactcorp.com/blog/Deep-Learning/rnn-vs-cnn-for-deep-learning\", \"content\": \"In this post will learn the difference between a deep learning RNN vs CNN. An important milestone in the history of deep learning was the introduction of the Recurrent Neural Network (RNN), which constituted a significant change in the makeup of the framework. It goes without question when comparing RNN vs CNN, both are commonplace in the field of Deep Learning. In this post will learn the difference between a deep learning RNN vs CNN. An important milestone in the history of deep learning was the introduction of the Recurrent Neural Network (RNN), which constituted a significant change in the makeup of the framework. It goes without question when comparing RNN vs CNN, both are commonplace in the field of Deep Learning.\", \"score\": 0.96885}]]\n",
      "\n",
      "Step 5\n",
      "----------------------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Let me provide a clearer, more focused explanation of how neural networks learn using the ImproveResponse function.\n",
      "Tool Calls:\n",
      "  ImproveResponse (toolu_bdrk_015Arb2eEPaF4ZB2T3WvcNUw)\n",
      " Call ID: toolu_bdrk_015Arb2eEPaF4ZB2T3WvcNUw\n",
      "  Args:\n",
      "    response: Neural networks learn by iteratively refining their predictions through a process called gradient descent. Here's how it works:\n",
      "\n",
      "Initially, the network starts with random connection weights between neurons. When data flows through these connections, each neuron performs two key operations: it multiplies incoming values by weights and applies an activation function (like ReLU or sigmoid) to introduce non-linearity [1].\n",
      "\n",
      "During training, the network:\n",
      "1. Makes predictions using current weights\n",
      "2. Calculates prediction errors using a loss function\n",
      "3. Uses backpropagation to compute how each weight contributed to errors\n",
      "4. Updates weights using optimizers like Adam, which automatically adjust learning rates [2]\n",
      "\n",
      "Modern networks employ crucial techniques to improve learning. Batch normalization prevents internal signals from becoming too extreme, while dropout randomly deactivates neurons to prevent overreliance on specific pathways [3]. Different architectures learn differently - CNNs excel at spatial patterns in images, while RNNs handle sequential data like text [4].\n",
      "\n",
      "The process repeats thousands of times, with the network gradually improving its predictions by adjusting weights based on the errors it encounters.\n",
      "    reflection: {'missing': \"The explanation could include more concrete examples of activation functions and their specific roles. It doesn't fully explain the mathematics behind gradient descent or how the network handles different types of data. The relationship between loss functions and model performance could be clearer.\", 'superfluous': 'The technical terminology might be overwhelming without proper context. The explanation of modern techniques could be more focused on their direct impact on learning rather than just listing them.'}\n",
      "    research_queries: ['practical examples of neural network activation functions and their effects', 'mathematical explanation of backpropagation and gradient descent', 'comparison of loss functions in neural network training']\n",
      "    sources: ['https://www.youtube.com/watch?v=pLf_W4OKxEQ', 'https://neptune.ai/blog/deep-learning-optimization-algorithms', 'https://milvus.io/ai-quick-reference/what-are-the-common-challenges-in-training-neural-networks', 'https://www.techtarget.com/searchenterpriseai/feature/CNN-vs-RNN-How-they-differ-and-where-they-overlap']\n",
      "\n",
      "Step 6\n",
      "----------------------------------------\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: ImproveResponse\n",
      "\n",
      "[[{\"title\": \"How Activation Functions Impact Neural Network Performance\", \"url\": \"https://businessanalyticsinstitute.com/activation-functions-impact-neural-network-performance/\", \"content\": \"Activation functions are an essential component of neural networks, responsible for introducing non-linearity and enabling the network to learn complex patterns.\\n Common types of activation functions include sigmoid, tanh, ReLU, Leaky ReLU, and softmax, each with its own characteristics and use cases.\\n Activation functions play a crucial role in determining the output of a neural network and can affect the model’s performance, convergence, and generalization capabilities. [...] Numerous case studies illustrate the profound impact that activation functions can have on neural network performance across various domains. In image recognition tasks, researchers have found that using ReLU significantly improves accuracy compared to traditional activation functions like sigmoid or tanh. For instance, convolutional neural networks (CNNs) designed for image classification have consistently demonstrated superior performance when employing ReLU or its variants. [...] The choice of activation function can significantly impact the training process of a neural network. Different functions have varying properties that can affect convergence speed and overall performance. For instance, ReLU has been shown to accelerate training in deep networks due to its ability to mitigate the vanishing gradient problem—a situation where gradients become too small for effective learning as they propagate back through layers during training.\", \"score\": 0.75049835}, {\"title\": \"Introduction to Activation Functions in Neural Networks - DataCamp\", \"url\": \"https://www.datacamp.com/tutorial/introduction-to-activation-functions-in-neural-networks\", \"content\": \"Neural networks leverage various types of activation functions to introduce non-linearities and enable learning complex patterns. Each activation function has its own unique properties and is suitable for certain use cases. \\n\\nFor example, the sigmoid function is ideal for binary classification problems, softmax is useful for multi-class prediction, and ReLU helps overcome the vanishing gradient problem. [...] Most real-world data is non-linear. For example, relationships between house prices and size, income, and purchases, etc., are non-linear. If neural networks had no activation functions, they would fail to learn the complex non-linear patterns that exist in real-world data. [...] We have explored the pivotal role activation functions play in the training of neural networks. We have seen how they are not just optional extras but essential elements that enable neural networks to capture and model the complexity inherent in real-world data. From the simple yet effective ReLU to the probabilistic interpretations provided by the softmax function, each activation function has its place and purpose within different layers of a network and across various problem domains.\", \"score\": 0.7343678}, {\"title\": \"Activation Functions in Neural Networks: 15 examples - Encord\", \"url\": \"https://encord.com/blog/activation-functions-neural-networks/\", \"content\": \"In this article, we explore 15 examples of activation functions and how they can be used to train neural networks.\\n\\n## Why Neural Networks Need Activation Functions?\\n\\nActivation functions are necessary for neural networks because, without them, the output of the model would simply be a linear function of the input. In other words, it wouldn’t be able to handle large volumes of complex data. Activation functions are an additional step in each forward propagation layer but a valuable one. [...] Activation functions in their numerous forms are mathematical equations that perform a vital function in a wide range of algorithmic and machine learning neural networks. Activation functions activate a neural network's problem-solving abilities, usually in the hidden layers, acting as gateway nodes between one layer and the next. [...] Activation functions play a crucial role in neural networks, performing a vital function in hidden layers to solve complex problems and to analyze and transmit data throughout deep learning algorithms. There are dozens of activation functions, including binary, linear, and numerous non-linear variants.\\n\\nThe activation function defines the output of a node based on a set of specific inputs in machine learning, deep neural networks, and artificial neural networks.\", \"score\": 0.7237202}, {\"title\": \"5 Popular Neural Network Activation Functions and When to Use ...\", \"url\": \"https://www.quarkml.com/2023/04/5-popular-activation-functions-used-in-neural-networks.html\", \"content\": \"In conclusion, activation functions play a crucial role in the performance of neural networks. They introduce non-linearity, enabling neural networks to learn complex relationships between inputs and outputs. In this article, we have discussed five popular activation functions, namely Sigmoid, Tanh, ReLU, Softmax, Swish, and ELU, and when to use or avoid them. It is important to note that the choice of activation function depends on the nature of the problem, the architecture of the network, [...] This is because activation functions like ReLU help some neurons to be activated while others remain dormant. When this happens, the activated neurons can identify and capture the patterns in the data, leading to more precise classification results. Additionally, by activating a subset of neurons and suppressing the rest, the overall loss of the Neural Network can be significantly reduced, making it more effective.\", \"score\": 0.6914979}, {\"title\": \"Activation functions in neural networks [Updated 2024]\", \"url\": \"https://www.superannotate.com/blog/activation-functions-in-neural-networks\", \"content\": \"In convolutional neural networks, activation functions can be ReLU-based to increase the convergence speed. However, some architectures require specific activation functions. For example, recurrent neural network architectures andLong Short Term Memory architectures utilize the sigmoid function and tanh function, and their logic gate-like architecture wouldn't work with ReLU.\\n\\nSumming up [...] Neural network architecture, machine learning tasks, and many others have an impact on activation function selection. For example, if the task is binary classification then the sigmoid activation function is a good choice, but for the multi-class classification softmax function is better as it will output probability representation for each class. [...] To recap, activation functions are crucial for modern neural networks because they facilitate the learning of a much wider set of functions and can, thus, make the model learn to perform all sorts of complex tasks. The impressiveadvances in computer vision, natural language processing, time series, and many other fields would be nearly impossible without the opportunities created by non-linear activation functions. While exponent-based activation functions like the sigmoid and tanh functions\", \"score\": 0.65676886}], [{\"title\": \"Gradient Descent Algorithm in Machine Learning\", \"url\": \"https://www.geeksforgeeks.org/machine-learning/gradient-descent-algorithm-and-its-variants/\", \"content\": \"Forward Propagation: Computes the output for a given input by passing data through the layers.\\n Backward Propagation: Uses the chain rule to calculate gradients of the loss with respect to each parameter (weights and biases) across all layers.\\n\\nGradients are then used by Gradient Descent to update the parameters layer-by-layer, moving toward minimizing the loss function. [...] Prerequisites: Understand the working and math of gradient descent.\\n\\n### 1. Training Machine Learning Models\\n\\nNeural networks are trained using Gradient Descent (or its variants) in combination with backpropagation. Backpropagation computes the gradients of the loss function with respect to each parameter (weights and biases) in the network by applying the chain rule. The process involves: [...] Now to update the weight and bias value and find the optimal value of weight and bias we will do backpropagation. Here the Gradient Descent comes into the role to find the optimal value weight and bias.\\n\\n## How the Gradient Descent Algorithm Works\\n\\nFor the sake of complexity, we can write our loss function for the single row as below\", \"score\": 0.7487387}, {\"title\": \"Backpropagation – The Math Behind Optimization - 365 Data Science\", \"url\": \"https://365datascience.com/trending/backpropagation/\", \"content\": \"Backpropagation—short for ‘backward propagation of errors’—is an optimization algorithm used to improve the accuracy of artificial neural networks. It’s an essential component of the gradient descent optimization process.\\n\\n### Deep Neural Network Components\\n\\nBefore diving into the math behind backpropagation, we must understand deep neural networks’ components. In the following example, we provide several denotations in the computations below. [...] In supervised learning, the optimization process consists of minimizing the loss. The idea of backpropagation is to compute the gradient of the loss function concerning the weights and biases of each unit in the network. Then, we use the gradients obtained to update the parameters such that the loss we compute at the new value is less than the loss at the current value. The loss decreases by iteratively adjusting the weights and biases based on the obtained gradients, and the network gradually [...] Forward propagation is the process of pushing inputs through the net. At the end of each epoch, we compare the obtained outputs to the targets to form the errors. In backpropagation, we reverse the process and adjust the weights and biases based on the obtained errors, minimizing the loss.\", \"score\": 0.7325349}, {\"title\": \"Backpropagation, intuitively | Deep Learning Chapter 3 - YouTube\", \"url\": \"https://www.youtube.com/watch?v=Ilg3gGewQ5U&vl=en\", \"content\": \"stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of [...] as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient [...] proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. Stochastic gradient descent By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training\", \"score\": 0.6729558}, {\"title\": \"Gradient descent\", \"url\": \"https://en.wikipedia.org/wiki/Gradient_descent\", \"content\": \"Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function.\", \"score\": 0.6453216}, {\"title\": \"Linear regression: Gradient descent | Machine Learning\", \"url\": \"https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent\", \"content\": \"On this page\\n Click the plus icon to learn more about the math behind gradient descent.\\n Model convergence and loss curves\\n  + Exercise: Check your understanding\\n  + Convergence and convex functions\\n\\nGradient descent is a\\nmathematical technique that iteratively finds the weights and bias that produce\\nthe model with the lowest loss. Gradient descent finds the best weight and bias\\nby repeating the following process for a number of user-defined iterations.\", \"score\": 0.6191726}], [{\"title\": \"Loss Functions and Their Use In Neural Networks\", \"url\": \"https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9/\", \"content\": \"A loss function is a function that compares the target and predicted output values; measures how well the neural network models the training data. When training, we aim to minimize this loss between the predicted and target outputs.\\n\\nThe hyperparameters are adjusted to minimize the average loss – we find the weights, wT, and biases, b, that minimize the value of J (average loss). [...] Each training input is loaded into the neural network in a process called forward propagation. Once the model has produced an output, this predicted output is compared against the given target output in a process called backpropagation – the hyperparameters of the model are then adjusted so that it now outputs a result closer to the target output.\\n\\nThis is where loss functions come in.\\n\\n## Loss Functions Overview [...] In supervised learning, there are two main types of loss functions – these correlate to the 2 major types of neural networks: regression and classification loss functions\", \"score\": 0.8194462}, {\"title\": \"Loss Functions in Deep Learning: A Comprehensive Review - arXiv\", \"url\": \"https://arxiv.org/html/2504.04242v1\", \"content\": \"Loss functions in deep learning play a critical role in optimizing neural networks during training by measuring the discrepancy between predicted outputs and actual ground truth labels . These functions are categorized into various types based on the task used. For example, for regression tasks involving continuous data, popular loss functions include Mean Squared Error (MSE) and Mean Absolute Error (MAE), which quantify the difference between predicted values and true labels. On the other [...] we compare the loss functions based on the architecture used including CNN-based models and ViT-based models. The methodology is presented in sections 4 and 5 for the used loss functions for the prediction from tabular data, and time series forecasting. A discussion of the computational evaluation of loss functions is presented in section 6. While, in section 7, we presented the advantages and disadvantages of some selected loss functions, challenges, and future directions. The conclusion is [...] functions. For example, Generative Adversarial Networks (GANs) use adversarial loss to train a generator to produce realistic images that fool a discriminator. While, autoencoders employ reconstruction loss to ensure that generated images closely resemble the original inputs. Thus, classification loss functions prioritize prediction accuracy, while generative loss functions focus on creating outputs that mirror real data.\", \"score\": 0.69672287}, {\"title\": \"Loss Functions in Neural Networks & Deep Learning\", \"url\": \"https://builtin.com/machine-learning/loss-functions\", \"content\": \"We only use the cross-entropy loss function in classification tasks when we want the neural network to predict probabilities.\\n\\nFor regression tasks, when we want the network to predict continuous numbers, we must use the mean squared error loss function.\\n\\nWe use mean absolute percentage error loss function during demand forecasting to keep an eye on the performance of the network during training time.\\n\\nRecent Artificial Intelligence Articles [...] 3 Key Types of Loss Functions in Neural Networks\\n\\nThat said, you still have to select the right loss function for the task at hand. Luckily there are only three loss functions you need to know to solve almost any problem.\\n\\n3 Key Loss Functions\\n\\n1. Mean Squared Error Loss Function\\n\\nMean squared error (MSE) loss function is the sum of squared differences between the entries in the prediction vector y and the ground truth vector y_hat. [...] Best Practices for Loss Functions and Neural Networks\\n\\nWhen you’re working with loss functions, just remember these key principles:\\n\\nA loss function measures how good a neural network model is in performing a certain task, which in most cases is regression or classification.\\n\\nWe must minimize the value of the loss function during the backpropagation step in order to make the neural network better.\", \"score\": 0.69199765}, {\"title\": \"How to Choose Loss Functions When Training Deep Learning ...\", \"url\": \"https://www.machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\", \"content\": \"A line plot is also created showing the mean squared error loss over the training epochs for both the train (blue) and test (orange) sets.\\n\\nWe can see that the model converged reasonably quickly and both train and test performance remained equivalent. The performance and convergence behavior of the model suggest that mean squared error is a good match for a neural network learning this problem. [...] Now that we have the basis of a problem and model, we can take a look evaluating three common loss functions that are appropriate for a multi-class classification predictive modeling problem.\\n\\nAlthough an MLP is used in these examples, the same loss functions can be used when training CNN and RNN models for multi-class classification.\\n\\n### Multi-Class Cross-Entropy Loss\\n\\nCross-entropy is the default loss function to use for multi-class classification problems. [...] Now that we have the basis of a problem and model, we can take a look evaluating three common loss functions that are appropriate for a binary classification predictive modeling problem.\\n\\nAlthough an MLP is used in these examples, the same loss functions can be used when training CNN and RNN models for binary classification.\\n\\n### Binary Cross-Entropy Loss\\n\\nCross-entropy is the default loss function to use for binary classification problems.\", \"score\": 0.5815176}, {\"title\": \"Types of Loss Functions in Machine Learning and Their ...\", \"url\": \"https://medium.com/@AIchemizt/types-of-loss-functions-in-machine-learning-and-their-usage-d4594e1529d2\", \"content\": \"Function: `nn.KLDivLoss()`\\n Usage: Used for comparing how one probability distribution differs from a second, expected probability distribution.\\n Explanation: This loss function measures the difference between two probability distributions. It’s often used in tasks where you want to compare predicted distributions to actual ones.\", \"score\": 0.50065976}]]\n",
      "\n",
      "Step 7\n",
      "----------------------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'll provide a final, refined explanation of how neural networks learn, focusing on clarity, technical accuracy, and accessibility.\n",
      "Tool Calls:\n",
      "  ImproveResponse (toolu_bdrk_0131adwc1rPwf6SjUofsiaM5)\n",
      " Call ID: toolu_bdrk_0131adwc1rPwf6SjUofsiaM5\n",
      "  Args:\n",
      "    response: Neural networks learn through a systematic process of trial, error, and optimization. Here's the core mechanism:\n",
      "\n",
      "When training begins, the network processes input data through layers of interconnected neurons, each applying two operations: a weighted sum of inputs and an activation function (like ReLU or sigmoid) that introduces non-linearity [1]. Initially, these weights are random, leading to poor predictions.\n",
      "\n",
      "The learning happens through three key steps:\n",
      "1. The network calculates its error using a loss function - Mean Squared Error for regression tasks or Cross-Entropy for classification [2]\n",
      "2. Through backpropagation, it computes how each connection contributed to the error, using calculus to find the gradient of the loss function [3]\n",
      "3. An optimizer like Adam or SGD updates the weights to reduce future errors, with the learning rate controlling the size of these adjustments [4]\n",
      "\n",
      "Modern techniques enhance this process: batch normalization stabilizes learning by normalizing layer inputs, while dropout prevents overfitting by randomly deactivating neurons during training [5]. Through thousands of iterations, the network gradually improves its predictions by finding optimal weight values.\n",
      "    reflection: {'missing': 'The explanation could benefit from more specific examples of real-world applications and how different types of networks (CNNs vs RNNs) learn differently. It could also explain the intuition behind why certain activation functions are chosen for specific tasks.', 'superfluous': 'The technical details about modern techniques like batch normalization could be simplified or removed to focus more on the core learning process.'}\n",
      "    research_queries: ['practical examples of neural network applications and their learning processes', 'comparison of learning mechanisms in different neural network architectures', 'how to choose activation functions for specific neural network tasks']\n",
      "    sources: ['https://www.youtube.com/watch?v=pLf_W4OKxEQ', 'https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9', 'https://365datascience.com/trending/backpropagation/', 'https://neptune.ai/blog/deep-learning-optimization-algorithms', 'https://milvus.io/ai-quick-reference/what-are-the-common-challenges-in-training-neural-networks']\n",
      "\n",
      "============================================================\n",
      "Reflexion agent execution completed!\n"
     ]
    }
   ],
   "source": [
    "# Run the agent with the neural networks question\n",
    "target_question = \"How do neural networks actually learn?\"\n",
    "\n",
    "print(f\"Running Reflexion agent with question: {target_question}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "events = reflexion_workflow.stream(\n",
    "    {\"messages\": [(\"user\", target_question)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "for i, step in enumerate(events):\n",
    "    print(f\"\\nStep {i}\")\n",
    "    print(\"-\" * 40)\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Reflexion agent execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd2936",
   "metadata": {},
   "source": [
    "The Reflexion agent will:\n",
    "- Generate an initial technical explanation with self-critique\n",
    "- Identify specific knowledge gaps requiring research\n",
    "- Execute targeted web searches for current information\n",
    "- Integrate findings into a comprehensive, cited response\n",
    "- Repeat the process until the explanation meets quality standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9072d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-uv",
   "language": "python",
   "name": "ai-agents-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
